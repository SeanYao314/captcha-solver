{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(f\"Number of examples: {len(train_images)}\")\n",
    "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
    "\n",
    "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
    "train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "    x = layers.Cropping2D((2, 2))(x)\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.utils.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.utils.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for training.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
    "\n",
    "# Get the wgan model\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = keras.saving.load_model(\"/Users/seanyao/cs/ML Keras/GAN MNIST/generator_mnist_wgan.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = np.random.randn(128)\n",
    "interpolate = np.random.randn(128)\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "weights = np.linspace(0, 1, num_steps)[:, np.newaxis]\n",
    "\n",
    "smooth_interpolation = seed * (1 - weights) + interpolate * weights\n",
    "generated_images = g_model.predict(smooth_interpolation, verbose=0)  # Generate an image\n",
    "generated_images = np.maximum(generated_images, 0)\n",
    "\n",
    "def showImage(arr):\n",
    "    plt.imshow(arr, cmap = 'gray')\n",
    "    plt.show()\n",
    "showImage(generated_images[0])\n",
    "showImage(generated_images[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming generated_images is a list of numpy arrays representing images\n",
    "# Each image has shape (28, 28)\n",
    "\n",
    "# Convert images to uint8 format\n",
    "images_uint8 = [(255 * image).astype(np.uint8) for image in generated_images]\n",
    "\n",
    "# Stack images along the first axis to ensure the shape is (num_images, 28, 28)\n",
    "images_stacked = np.stack(images_uint8)\n",
    "\n",
    "# Convert grayscale images to RGB format (replicate along the third axis to create three identical channels)\n",
    "images_rgb = np.repeat(images_stacked[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Create a list to store the images\n",
    "images_list = []\n",
    "\n",
    "# Convert each image to PIL Image and append to the list\n",
    "for image in images_rgb:\n",
    "    # Ensure the image has the correct shape (height, width, channels)\n",
    "    image = image.squeeze()  # Remove the single-channel dimension if present\n",
    "    pil_image = Image.fromarray(image, mode='RGB')\n",
    "    images_list.append(pil_image)\n",
    "\n",
    "# Save the list of images as an animated GIF using imageio\n",
    "with imageio.get_writer('wgan-gp-latentspace-smooth-interpolation1.gif', mode='I', duration=0.01) as writer:\n",
    "    for image in images_list:\n",
    "        writer.append_data(np.array(image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
