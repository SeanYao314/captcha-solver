{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 19:21:49.887230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(f\"Number of examples: {len(train_images)}\")\n",
    "print(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n",
    "\n",
    "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
    "train_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "    x = layers.Cropping2D((2, 2))(x)\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.utils.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.utils.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "\n",
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for training.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
    "\n",
    "# Get the wgan model\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = keras.saving.load_model(\"/Users/seanyao/cs/ML Keras/GAN MNIST/generator_mnist_wgan.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbSklEQVR4nO3df2xV9f3H8ddtSy+g7WWltLdXChRQ2OSHkUHXoQxCA3SGAGICziy4EAissCHzR1gmKFvSjSVqXBguWQIzE1SSARETFi22xFlAfo24jYZiXUugRdm4txRbSPv5/sHXO6+04Lnc23d/PB/JJ+k957zvefPhcF899x7O9TnnnAAA6GIp1g0AAPomAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm0qwb+Kr29nadO3dOGRkZ8vl81u0AADxyzqmpqUmhUEgpKZ2f53S7ADp37pzy8/Ot2wAA3Kb6+noNHTq00/Xd7i24jIwM6xYAAAlwq9fzpAXQ5s2bNWLECPXv31+FhYU6fPjw16rjbTcA6B1u9XqelAB64403tHbtWm3YsEHHjh3TxIkTNXv2bF24cCEZuwMA9EQuCaZMmeJKS0ujj9va2lwoFHJlZWW3rA2Hw04Sg8FgMHr4CIfDN329T/gZ0NWrV3X06FEVFxdHl6WkpKi4uFhVVVU3bN/a2qpIJBIzAAC9X8ID6LPPPlNbW5tyc3Njlufm5qqhoeGG7cvKyhQIBKKDK+AAoG8wvwpu3bp1CofD0VFfX2/dEgCgCyT8/wFlZ2crNTVVjY2NMcsbGxsVDAZv2N7v98vv9ye6DQBAN5fwM6D09HRNmjRJ5eXl0WXt7e0qLy9XUVFRoncHAOihknInhLVr12rJkiX69re/rSlTpuill15Sc3OzfvSjHyVjdwCAHigpAbRo0SJ9+umnWr9+vRoaGnTfffdp3759N1yYAADou3zOOWfdxJdFIhEFAgHrNgAAtykcDiszM7PT9eZXwQEA+iYCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYSHgAPffcc/L5fDFj7Nixid4NAKCHS0vGk95777169913/7eTtKTsBgDQgyUlGdLS0hQMBpPx1ACAXiIpnwGdPn1aoVBII0eO1GOPPaa6urpOt21tbVUkEokZAIDeL+EBVFhYqG3btmnfvn3asmWLamtr9eCDD6qpqanD7cvKyhQIBKIjPz8/0S0BALohn3POJXMHly5d0vDhw/XCCy9o6dKlN6xvbW1Va2tr9HEkEiGEAKAXCIfDyszM7HR90q8OGDRokO655x7V1NR0uN7v98vv9ye7DQBAN5P0/wd0+fJlnTlzRnl5ecneFQCgB0l4AD355JOqrKzUJ598og8++EALFixQamqqHn300UTvCgDQgyX8LbizZ8/q0Ucf1cWLFzVkyBA98MADOnjwoIYMGZLoXQEAerCkX4TgVSQSUSAQsG4DAHCbbnURAveCAwCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLpX0gHwE4oFIqrLi3N+0tDXV1dXPtC38UZEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABHfDBr5k3LhxnmsmTZrkuWbBggWea2bOnOm5pr293XONJLW0tMRV55Xf7/dc09TU5LmmoaHBc40k/fGPf/Rc8+abb3quWb16teeavXv3eq6RpGPHjsVVlwycAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDhc8456ya+LBKJKBAIWLeBbiQ3N9dzzTPPPBPXvr773e96rqmvr/dck5Li/Xe/oUOHeq6J98aT8dyU9f777/dcM2DAAM81Pp/Pc013F8/LcGtra1z7Wrx4seeaPXv2xLWvcDiszMzMTtdzBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBEmnUD6Fv+8Y9/eK4ZPXq055p4b9T497//3XPNs88+67nm1KlTnmuWLVvmuSae+Zak/Px8zzWbN2/2XOP3+z3XpKameq7pSu3t7Z5r4jleX3zxRc81Uvw3Fk0GzoAAACYIIACACc8BdODAAc2dO1ehUEg+n0+7d++OWe+c0/r165WXl6cBAwaouLhYp0+fTlS/AIBewnMANTc3a+LEiZ2+37tp0ya9/PLLeuWVV3To0CHdcccdmj17tlpaWm67WQBA7+H5IoSSkhKVlJR0uM45p5deekm/+MUvNG/ePEnSq6++qtzcXO3evTuub+IDAPROCf0MqLa2Vg0NDSouLo4uCwQCKiwsVFVVVYc1ra2tikQiMQMA0PslNIAaGhokSbm5uTHLc3Nzo+u+qqysTIFAIDriufwTANDzmF8Ft27dOoXD4eior6+3bgkA0AUSGkDBYFCS1NjYGLO8sbExuu6r/H6/MjMzYwYAoPdLaAAVFBQoGAyqvLw8uiwSiejQoUMqKipK5K4AAD2c56vgLl++rJqamujj2tpanThxQllZWRo2bJjWrFmjX/3qV7r77rtVUFCgZ599VqFQSPPnz09k3wCAHs5zAB05ckQzZsyIPl67dq0kacmSJdq2bZuefvppNTc3a/ny5bp06ZIeeOAB7du3T/37909c1wCAHs9zAE2fPl3OuU7X+3w+bdy4URs3brytxtA7DRs2zHNNSor3d4qbm5s910jShx9+6Lnmvvvu81zz8ccfd0nNJ5984rlGkgoLCz3XxHMTznA47LnmP//5j+eaH/7wh55rJOngwYNx1eHrMb8KDgDQNxFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATPjczW5tbSASiSgQCFi3gSS5ePGi55oBAwZ0yX4kKS3N8w3iu+x4bWtr81xz4sSJuPZ1/PhxzzXx3Omc7wnr3cLh8E2/5ZozIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACa833kRuA1Tp071XLNv3z7PNbm5uZ5rJMnv93uu8fl8ce3Lq3juG1xYWBjXvu644w7PNY888khc+0LfxRkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE9yMFF3q1KlTnmtGjBiR+EY68fbbb3uuycnJ8VzTv39/zzUDBw70XDN48GDPNZL0/vvve675+OOP49oX+i7OgAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjgZqTAlzz00EPWLXRqwoQJnms+/PDDuPaVlsZLA5KPMyAAgAkCCABgwnMAHThwQHPnzlUoFJLP59Pu3btj1j/++OPy+XwxY86cOYnqFwDQS3gOoObmZk2cOFGbN2/udJs5c+bo/Pnz0bFjx47bahIA0Pt4/qSxpKREJSUlN93G7/crGAzG3RQAoPdLymdAFRUVysnJ0ZgxY7Ry5UpdvHix021bW1sViURiBgCg90t4AM2ZM0evvvqqysvL9Zvf/EaVlZUqKSlRW1tbh9uXlZUpEAhER35+fqJbAgB0Qwm/2H/x4sXRn8ePH68JEyZo1KhRqqio0MyZM2/Yft26dVq7dm30cSQSIYQAoA9I+mXYI0eOVHZ2tmpqajpc7/f7lZmZGTMAAL1f0gPo7NmzunjxovLy8pK9KwBAD+L5LbjLly/HnM3U1tbqxIkTysrKUlZWlp5//nktXLhQwWBQZ86c0dNPP63Ro0dr9uzZCW0cANCzeQ6gI0eOaMaMGdHHX3x+s2TJEm3ZskUnT57Un/70J126dEmhUEizZs3SL3/5S/n9/sR1DQDo8TwH0PTp0+Wc63T9X//619tqCEDHfvKTn3iu6devX1z7Gjx4cFx1gBfcCw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYMLnbnZrawORSESBQMC6DaDbuXbtmuea1NTUuPaVksLvprh94XD4pt9yzVEGADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARJp1A+hcenq655pvfetbnmtOnDjhuQa3p7W11XNNWpr3f64ffPCB5xqgq3AGBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQ3I+3G3n77bc81M2bMSEInHTt+/LjnmsmTJyehk8QZN26c55rDhw97ronnRrNtbW2ea6ZOneq5BugqnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw4XPOOesmviwSiSgQCFi30S2EQiHPNfX19Z5rUlK67veQeG6oGU9NvH+meOriqYnnn11X/j0BiRAOh5WZmdnpeo5oAIAJAggAYMJTAJWVlWny5MnKyMhQTk6O5s+fr+rq6phtWlpaVFpaqsGDB+vOO+/UwoUL1djYmNCmAQA9n6cAqqysVGlpqQ4ePKh33nlH165d06xZs9Tc3Bzd5oknntBbb72lnTt3qrKyUufOndPDDz+c8MYBAD3bbV2E8OmnnyonJ0eVlZWaNm2awuGwhgwZou3bt+uRRx6RJJ06dUrf/OY3VVVVpe985zu3fE4uQvgfLkKIv4aLEAB7Sb0IIRwOS5KysrIkSUePHtW1a9dUXFwc3Wbs2LEaNmyYqqqqOnyO1tZWRSKRmAEA6P3iDqD29natWbNGU6dO1bhx4yRJDQ0NSk9P16BBg2K2zc3NVUNDQ4fPU1ZWpkAgEB35+fnxtgQA6EHiDqDS0lJ99NFHev3112+rgXXr1ikcDkdHPG8hAQB6nrR4ilatWqW9e/fqwIEDGjp0aHR5MBjU1atXdenSpZizoMbGRgWDwQ6fy+/3y+/3x9MGAKAH83QG5JzTqlWrtGvXLu3fv18FBQUx6ydNmqR+/fqpvLw8uqy6ulp1dXUqKipKTMcAgF7B0xlQaWmptm/frj179igjIyP6uU4gENCAAQMUCAS0dOlSrV27VllZWcrMzNTq1atVVFT0ta6AAwD0HZ4CaMuWLZKk6dOnxyzfunWrHn/8cUnSiy++qJSUFC1cuFCtra2aPXu2fv/73yekWQBA78HNSHuZeC7i6OzzuVtpamryXBPP/2W5evWq55rU1FTPNVJ8/cVzp4+xY8d6rgF6Gm5GCgDolgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJuL6RlR0X/n5+dYt3FQ8d4H+8rfrfl3//e9/PddI179AEUDX4AwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACW5Gii516tQp6xYAdBOcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw4SmAysrKNHnyZGVkZCgnJ0fz589XdXV1zDbTp0+Xz+eLGStWrEho0wCAns9TAFVWVqq0tFQHDx7UO++8o2vXrmnWrFlqbm6O2W7ZsmU6f/58dGzatCmhTQMAer40Lxvv27cv5vG2bduUk5Ojo0ePatq0adHlAwcOVDAYTEyHAIBe6bY+AwqHw5KkrKysmOWvvfaasrOzNW7cOK1bt05Xrlzp9DlaW1sViURiBgCgD3Bxamtrcw899JCbOnVqzPI//OEPbt++fe7kyZPuz3/+s7vrrrvcggULOn2eDRs2OEkMBoPB6GUjHA7fNEfiDqAVK1a44cOHu/r6+ptuV15e7iS5mpqaDte3tLS4cDgcHfX19eaTxmAwGIzbH7cKIE+fAX1h1apV2rt3rw4cOKChQ4fedNvCwkJJUk1NjUaNGnXDer/fL7/fH08bAIAezFMAOee0evVq7dq1SxUVFSooKLhlzYkTJyRJeXl5cTUIAOidPAVQaWmptm/frj179igjI0MNDQ2SpEAgoAEDBujMmTPavn27vv/972vw4ME6efKknnjiCU2bNk0TJkxIyh8AANBDefncR528z7d161bnnHN1dXVu2rRpLisry/n9fjd69Gj31FNP3fJ9wC8Lh8Pm71syGAwG4/bHrV77ff8fLN1GJBJRIBCwbgMAcJvC4bAyMzM7Xc+94AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJrpdADnnrFsAACTArV7Pu10ANTU1WbcAAEiAW72e+1w3O+Vob2/XuXPnlJGRIZ/PF7MuEokoPz9f9fX1yszMNOrQHvNwHfNwHfNwHfNwXXeYB+ecmpqaFAqFlJLS+XlOWhf29LWkpKRo6NChN90mMzOzTx9gX2AermMermMermMerrOeh0AgcMttut1bcACAvoEAAgCY6FEB5Pf7tWHDBvn9futWTDEP1zEP1zEP1zEP1/Wkeeh2FyEAAPqGHnUGBADoPQggAIAJAggAYIIAAgCY6DEBtHnzZo0YMUL9+/dXYWGhDh8+bN1Sl3vuuefk8/lixtixY63bSroDBw5o7ty5CoVC8vl82r17d8x655zWr1+vvLw8DRgwQMXFxTp9+rRNs0l0q3l4/PHHbzg+5syZY9NskpSVlWny5MnKyMhQTk6O5s+fr+rq6phtWlpaVFpaqsGDB+vOO+/UwoUL1djYaNRxcnydeZg+ffoNx8OKFSuMOu5YjwigN954Q2vXrtWGDRt07NgxTZw4UbNnz9aFCxesW+ty9957r86fPx8d77//vnVLSdfc3KyJEydq8+bNHa7ftGmTXn75Zb3yyis6dOiQ7rjjDs2ePVstLS1d3Gly3WoeJGnOnDkxx8eOHTu6sMPkq6ysVGlpqQ4ePKh33nlH165d06xZs9Tc3Bzd5oknntBbb72lnTt3qrKyUufOndPDDz9s2HXifZ15kKRly5bFHA+bNm0y6rgTrgeYMmWKKy0tjT5ua2tzoVDIlZWVGXbV9TZs2OAmTpxo3YYpSW7Xrl3Rx+3t7S4YDLrf/va30WWXLl1yfr/f7dixw6DDrvHVeXDOuSVLlrh58+aZ9GPlwoULTpKrrKx0zl3/u+/Xr5/buXNndJt//etfTpKrqqqyajPpvjoPzjn3ve99z/30pz+1a+pr6PZnQFevXtXRo0dVXFwcXZaSkqLi4mJVVVUZdmbj9OnTCoVCGjlypB577DHV1dVZt2SqtrZWDQ0NMcdHIBBQYWFhnzw+KioqlJOTozFjxmjlypW6ePGidUtJFQ6HJUlZWVmSpKNHj+ratWsxx8PYsWM1bNiwXn08fHUevvDaa68pOztb48aN07p163TlyhWL9jrV7W5G+lWfffaZ2tralJubG7M8NzdXp06dMurKRmFhobZt26YxY8bo/Pnzev755/Xggw/qo48+UkZGhnV7JhoaGiSpw+Pji3V9xZw5c/Twww+roKBAZ86c0c9//nOVlJSoqqpKqamp1u0lXHt7u9asWaOpU6dq3Lhxkq4fD+np6Ro0aFDMtr35eOhoHiTpBz/4gYYPH65QKKSTJ0/qmWeeUXV1tf7yl78Ydhur2wcQ/qekpCT684QJE1RYWKjhw4frzTff1NKlSw07Q3ewePHi6M/jx4/XhAkTNGrUKFVUVGjmzJmGnSVHaWmpPvrooz7xOejNdDYPy5cvj/48fvx45eXlaebMmTpz5oxGjRrV1W12qNu/BZedna3U1NQbrmJpbGxUMBg06qp7GDRokO655x7V1NRYt2Lmi2OA4+NGI0eOVHZ2dq88PlatWqW9e/fqvffei/n6lmAwqKtXr+rSpUsx2/fW46GzeehIYWGhJHWr46HbB1B6eromTZqk8vLy6LL29naVl5erqKjIsDN7ly9f1pkzZ5SXl2fdipmCggIFg8GY4yMSiejQoUN9/vg4e/asLl682KuOD+ecVq1apV27dmn//v0qKCiIWT9p0iT169cv5niorq5WXV1drzoebjUPHTlx4oQkda/jwfoqiK/j9ddfd36/323bts3985//dMuXL3eDBg1yDQ0N1q11qZ/97GeuoqLC1dbWur/97W+uuLjYZWdnuwsXLli3llRNTU3u+PHj7vjx406Se+GFF9zx48fdv//9b+ecc7/+9a/doEGD3J49e9zJkyfdvHnzXEFBgfv888+NO0+sm81DU1OTe/LJJ11VVZWrra117777rrv//vvd3Xff7VpaWqxbT5iVK1e6QCDgKioq3Pnz56PjypUr0W1WrFjhhg0b5vbv3++OHDniioqKXFFRkWHXiXereaipqXEbN250R44ccbW1tW7Pnj1u5MiRbtq0acadx+oRAeScc7/73e/csGHDXHp6upsyZYo7ePCgdUtdbtGiRS4vL8+lp6e7u+66yy1atMjV1NRYt5V07733npN0w1iyZIlz7vql2M8++6zLzc11fr/fzZw501VXV9s2nQQ3m4crV664WbNmuSFDhrh+/fq54cOHu2XLlvW6X9I6+vNLclu3bo1u8/nnn7sf//jH7hvf+IYbOHCgW7BggTt//rxd00lwq3moq6tz06ZNc1lZWc7v97vRo0e7p556yoXDYdvGv4KvYwAAmOj2nwEBAHonAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJv4PQK25x8SQWhsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = np.random.randn(128)\n",
    "interpolate = np.random.randn(128)\n",
    "\n",
    "num_steps = 100\n",
    "\n",
    "weights = np.linspace(0, 1, num_steps)[:, np.newaxis]\n",
    "\n",
    "smooth_interpolation = seed * (1 - weights) + interpolate * weights\n",
    "generated_images = g_model.predict(smooth_interpolation, verbose=0)  # Generate an image\n",
    "generated_images = np.maximum(generated_images, 0)\n",
    "\n",
    "def showImage(arr):\n",
    "    plt.imshow(arr, cmap = 'gray')\n",
    "    plt.show()\n",
    "showImage(generated_images[0])\n",
    "showImage(generated_images[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming generated_images is a list of numpy arrays representing images\n",
    "# Each image has shape (28, 28)\n",
    "\n",
    "# Convert images to uint8 format\n",
    "images_uint8 = [(255 * image).astype(np.uint8) for image in generated_images]\n",
    "\n",
    "# Stack images along the first axis to ensure the shape is (num_images, 28, 28)\n",
    "images_stacked = np.stack(images_uint8)\n",
    "\n",
    "# Convert grayscale images to RGB format (replicate along the third axis to create three identical channels)\n",
    "images_rgb = np.repeat(images_stacked[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Create a list to store the images\n",
    "images_list = []\n",
    "\n",
    "# Convert each image to PIL Image and append to the list\n",
    "for image in images_rgb:\n",
    "    # Ensure the image has the correct shape (height, width, channels)\n",
    "    image = image.squeeze()  # Remove the single-channel dimension if present\n",
    "    pil_image = Image.fromarray(image, mode='RGB')\n",
    "    images_list.append(pil_image)\n",
    "\n",
    "# Save the list of images as an animated GIF using imageio\n",
    "with imageio.get_writer('wgan-gp-latentspace-smooth-interpolation1.gif', mode='I', duration=0.01) as writer:\n",
    "    for image in images_list:\n",
    "        writer.append_data(np.array(image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
