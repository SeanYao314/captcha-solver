{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as im\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(arr):\n",
    "    plt.imshow(arr, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000,28,28,1))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28,28,1))\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    ")\n",
    "from keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Conv2D, Flatten, Dropout, LeakyReLU, MaxPooling2D, Conv2DTranspose, Reshape)\n",
    "\n",
    "def def_discriminator(in_shape=(28,28,1)):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def def_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))   \n",
    "    model.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Dense(784, activation=LeakyReLU(alpha=0.2)))\n",
    "    model.add(Reshape((28,28,1)))\n",
    "\n",
    "    return model\n",
    "\n",
    "# def def_generator(latent_dim):\n",
    "#     # Define the input layer\n",
    "#     input_layer = keras.layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "#     # Project and reshape the input\n",
    "#     dense_layer = Dense(7 * 7 * 128)(input_layer)\n",
    "#     # dense_layer = Reshape((7, 7, 128))(dense_layer)\n",
    "    \n",
    "#     # # Upsampling blocks\n",
    "#     # conv_transpose1 = Conv2DTranspose(64, (3, 3), padding='same', activation=LeakyReLU(alpha=0.2))(dense_layer)\n",
    "#     # conv_transpose1 = BatchNormalization()(conv_transpose1)\n",
    "    \n",
    "#     # conv_transpose2 = Conv2DTranspose(32, (3, 3), padding='same', activation=LeakyReLU(alpha=0.2))(conv_transpose1)\n",
    "#     # conv_transpose2 = BatchNormalization()(conv_transpose2)\n",
    "    \n",
    "\n",
    "#     x = Dense(784, activation=LeakyReLU(alpha=0.1))(dense_layer)\n",
    "#     x = Dense(784, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = Dense(784, activation=LeakyReLU(alpha=0.1))(x)\n",
    "#     x = Reshape((28,28,1))(x)\n",
    "    \n",
    "#     # Define the model\n",
    "#     model = keras.Model(inputs=input_layer, outputs=x)\n",
    "#     return model\n",
    "generator = def_generator(100)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_gan(generator, d):\n",
    "#     d.trainable = False\n",
    "#     model = Sequential()\n",
    "#     model.add(generator)\n",
    "#     model.add(d)\n",
    "#     opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "#     model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "#     return model\n",
    "from keras import backend as K\n",
    "def generator_loss(y_true, y_pred, generated_data):\n",
    "    # Calculate mean squared error between batches\n",
    "    similarity_penalty = tf.keras.backend.mean(K.square(K.mean(generated_data, axis=0) - generated_data), axis=None)\n",
    "    \n",
    "    return K.binary_crossentropy(y_true, y_pred) + similarity_penalty\n",
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=lambda y_true, y_pred: generator_loss(y_true, y_pred, generator.output), metrics=['accuracy'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100\n",
    "D_BATCH_SIZE = 10000\n",
    "G_BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "DATA_LEN = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_label(arr):\n",
    "    return arr.reshape(len(arr), 1)\n",
    "def shuffle(ims, labels):\n",
    "    if(len(ims)!=len(labels)):\n",
    "        return -1\n",
    "    rand_index = np.random.permutation(len(ims))\n",
    "    return ims[rand_index], labels[rand_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = def_discriminator()\n",
    "generator = def_generator(LATENT_DIM)\n",
    "gan = define_gan(generator, discriminator)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_latent_dims = np.random.randn(500, LATENT_DIM)\n",
    "gan.fit(g_latent_dims, re_label(np.ones(500)), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ims():\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(10, 10))  # Create a 5x5 subplot grid\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            generated_image = generator.predict(np.random.randn(1, LATENT_DIM)*2, verbose=0)  # Generate an image\n",
    "            axes[i, j].imshow(generated_image.reshape(28, 28), cmap='gray')  # Display the generated image\n",
    "            axes[i, j].axis('off')  # Turn off axis labels\n",
    "    plt.tight_layout()  # Adjust subplot layout\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(generator.predict(np.random.rand(1,100))[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable=True\n",
    "d_latent_dims = np.random.randn(1000//2, LATENT_DIM)\n",
    "\n",
    "rand_index = np.random.randint(0,DATA_LEN, 1000//2)\n",
    "\n",
    "real_ims, real_labels = train_images[rand_index], re_label(np.ones(1000//2))\n",
    "fake_ims, fake_labels = generator.predict(d_latent_dims, verbose=0), re_label(np.zeros(1000//2))\n",
    "\n",
    "\n",
    "ims, labels = shuffle(np.vstack((real_ims, fake_ims)), np.vstack((real_labels, fake_labels)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "discriminator.fit(ims,labels, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_latent_dims = np.random.randn(500, LATENT_DIM)*2\n",
    "gan.fit(g_latent_dims, re_label(np.ones(500)), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.load_weights(\"/Users/seanyao/cs/ML Keras/GAN MNIST/gan_FNN.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    for batch in tqdm(range(DATA_LEN//G_BATCH_SIZE)):\n",
    "        d_latent_dims = np.random.randn(D_BATCH_SIZE//2, LATENT_DIM)\n",
    "        g_latent_dims = np.random.randn(G_BATCH_SIZE, LATENT_DIM)\n",
    "\n",
    "        rand_index = np.random.randint(0,DATA_LEN, D_BATCH_SIZE//2)\n",
    "\n",
    "        real_ims, real_labels = train_images[rand_index], re_label(np.ones(D_BATCH_SIZE//2))\n",
    "        fake_ims, fake_labels = generator.predict(d_latent_dims, verbose=0), re_label(np.zeros(D_BATCH_SIZE//2))\n",
    "\n",
    "        ims, labels = shuffle(np.vstack((real_ims, fake_ims)), np.vstack((real_labels, fake_labels)))\n",
    "\n",
    "        # discriminator.fit(ims,labels, batch_size=32)\n",
    "        d_loss = discriminator.train_on_batch(ims, labels)\n",
    "        g_loss = gan.train_on_batch(g_latent_dims, re_label(np.ones(G_BATCH_SIZE)))\n",
    "\n",
    "        losses.append((d_loss, g_loss))\n",
    "        \n",
    "        print(\"Discriminator Loss:\", d_loss[0], \"Generator Loss: \", g_loss[0])\n",
    "        print(\"Discriminator Acc:\", d_loss[1], \"Generator Acc: \", g_loss[1])\n",
    "        if(batch%3==0):\n",
    "            clear_output()\n",
    "            plot_ims()\n",
    "        if(batch%10==0):\n",
    "            s = \"gan_FNN_more_layers.keras\"\n",
    "            gan.save(s)\n",
    "    print(\"Discriminator Loss:\", d_loss, \" \", g_loss, \"Generator Loss\")\n",
    "    plot_ims()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"gan_CNN.keras\"\n",
    "gan.save(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
